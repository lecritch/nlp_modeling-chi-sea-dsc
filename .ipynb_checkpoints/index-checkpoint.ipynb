{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and NLP Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before returning to our Satire/No Satire example, let's consider an example with a smaller but similar scope.\n",
    "\n",
    "Suppose we are using an API to gather articles from a news website and grabbing phrases from two different types of articles:  **music** and **politics**.\n",
    "\n",
    "We have a problem though! Only some of our articles have their category (music or politics). Is there a way we can use Machine Learning to help us label our data **quickly**?\n",
    "\n",
    "-------------------------------\n",
    "### Here are our articles\n",
    "#### Music Articles:\n",
    "\n",
    "* 'the song was popular'\n",
    "* 'band leaders disagreed on sound'\n",
    "* 'played for a sold out arena stadium'\n",
    "\n",
    "#### Politics Articles\n",
    "\n",
    "* 'world leaders met lask week'\n",
    "* 'the election was close'\n",
    "* 'the officials agreed on a compromise'\n",
    "--------------------------------------------------------\n",
    "Let's try and predict one example phrase:\n",
    "\n",
    "\n",
    "* \"world leaders agreed to fund the stadium\"\n",
    "\n",
    "How can we make a model that labels this for us rather than having to go through by hand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "music = ['the song was popular',\n",
    "         'band leaders disagreed on sound',\n",
    "         'played for a sold out arena stadium']\n",
    "\n",
    "politics = ['world leaders met lask week',\n",
    "            'the election was close',\n",
    "            'the officials agreed on a compromise']\n",
    "\n",
    "test_statement = 'world leaders agreed to fund the stadium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels : 'music' 'politics'\n",
    "#features: words\n",
    "test_statement_2 = 'officials met at the arena'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit Bayes Theorem.  Remember, Bayes looks to calculate the probability of a class (c) given the data (x).  To do so, we calculate the **likelihood** (the distribution of our data within a given class) and the **prior** probabiliity of each class (the probability of seeing the class in the population). We are going to ignore the denominator of the right side of the equation in this instance, because, as we will see, we will be finding the ratio of posteriors probabilities, which will cancel out the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"./resources/naive_bayes_icon.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of looking at it\n",
    "<img src = \"./resources/another_one.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, in the context of our problem......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$\\large P(politics | phrase) = \\frac{P(phrase|politics)P(politics)}{P(phrase)}$\n",
    "\n",
    "$\\large P(politics) = \\frac{ \\# politics}{\\# all\\ articles} $\n",
    "\n",
    "*where phrase is our test statement*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/solving_theta.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate P(politics)?\n",
    "\n",
    "This is essentially the distribution of the probability of either type of article. We have three of each type of article, therefore, we assume that there is an equal probability of either article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_politics = len(politics)/(len(politics) + len(music))\n",
    "p_music = len(music)/(len(politics) + len(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is always a good idea\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "mccalister = ['Adam', 'Amanda','Chum', 'Dann',\n",
    " 'Jacob', 'Jason', 'Johnhoy', 'Karim',\n",
    "'Leana','Luluva', 'Matt', 'Maximilian','Syd' ]\n",
    "\n",
    "from src.student_caller import one_random_student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you think we should calculate: $ P(phrase | politics) $ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Matt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# we need to break the phrases down into individual words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\large P(phrase | politics) = \\prod_{i=1}^{d} P(word_{i} | politics) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We need to make a *Naive* assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\large P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ art.} {\\#\\ of\\ total\\ words\\ in\\ politics\\ art.} $\n",
    "\n",
    "### Can you foresee any issues with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maximilian'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# we can't have a probability of 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing\n",
    " $\\large P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ art. + \\alpha} {\\#\\ of\\ total\\ words\\ in\\ politics\\ art. + \\alpha d} $\n",
    "\n",
    " $\\large P(word_{i} | music) = \\frac{\\#\\ of\\ word_{i}\\ in\\ music\\ art. + \\alpha} {\\#\\ of\\ total\\ words\\ in\\ music\\ art. + \\alpha d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correction process is called Laplace smoothing:\n",
    "* d : number of features (in this instance total number of vocabulary words)\n",
    "* $\\alpha$ can be any number greater than 0 (it is usually 1)\n",
    "\n",
    "\n",
    "#### Now let's find this calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/IMG_0041.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_maker(category):\n",
    "    \"\"\"\n",
    "    parameters: category is a list containing all the articles\n",
    "    of a given category.\n",
    "    \n",
    "    returns the vocabulary for a given type of article\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_category = set() # will filter down to only unique words\n",
    "    \n",
    "    for art in category:\n",
    "        words = art.split()\n",
    "        for word in words:\n",
    "            vocab_category.add(word)\n",
    "    return vocab_category\n",
    "        \n",
    "voc_music = vocab_maker(music)\n",
    "voc_pol = vocab_maker(politics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'arena',\n",
       " 'band',\n",
       " 'disagreed',\n",
       " 'for',\n",
       " 'leaders',\n",
       " 'on',\n",
       " 'out',\n",
       " 'played',\n",
       " 'popular',\n",
       " 'sold',\n",
       " 'song',\n",
       " 'sound',\n",
       " 'stadium',\n",
       " 'the',\n",
       " 'was'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are all the unique words in the music category\n",
    "voc_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'agreed',\n",
       " 'close',\n",
       " 'compromise',\n",
       " 'election',\n",
       " 'lask',\n",
       " 'leaders',\n",
       " 'met',\n",
       " 'officials',\n",
       " 'on',\n",
       " 'the',\n",
       " 'was',\n",
       " 'week',\n",
       " 'world'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are all the unique words in the politics category\n",
    "voc_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'agreed',\n",
       " 'arena',\n",
       " 'band',\n",
       " 'close',\n",
       " 'compromise',\n",
       " 'disagreed',\n",
       " 'election',\n",
       " 'for',\n",
       " 'lask',\n",
       " 'leaders',\n",
       " 'met',\n",
       " 'officials',\n",
       " 'on',\n",
       " 'out',\n",
       " 'played',\n",
       " 'popular',\n",
       " 'sold',\n",
       " 'song',\n",
       " 'sound',\n",
       " 'stadium',\n",
       " 'the',\n",
       " 'was',\n",
       " 'week',\n",
       " 'world'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The union of the two sets gives us the unique words across both article groups\n",
    "voc_all = voc_music.union(voc_pol)\n",
    "voc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_count = len(voc_all)\n",
    "total_music_count = len(voc_music)\n",
    "total_politics_count = len(voc_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of the goal, to see the posterior likelihood of the class politics given our phrase. \n",
    "\n",
    "> P(politics | leaders agreed to fund the stadium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the song was popular',\n",
       " 'band leaders disagreed on sound',\n",
       " 'played for a sold out arena stadium']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_words_in_category(phrase,category):\n",
    "    statement = phrase.split()\n",
    "    \n",
    "    # category is a list of the raw documents of each category\n",
    "    str_category=' '.join(category)\n",
    "    cat_word_list = str_category.split()\n",
    "    word_count = defaultdict(int)\n",
    "    \n",
    "    # loop through each word in the phrase\n",
    "    for word in statement:\n",
    "        # loop through each word in the category\n",
    "        for art_word in cat_word_list:\n",
    "            if word == art_word:\n",
    "                # count the number of times the phrase word occurs in the category\n",
    "                word_count[word] +=1\n",
    "            else:\n",
    "                word_count[word]\n",
    "    return word_count\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_music_word_count = find_number_words_in_category(test_statement,music)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'world': 0,\n",
       "             'leaders': 1,\n",
       "             'agreed': 0,\n",
       "             'to': 0,\n",
       "             'fund': 0,\n",
       "             'the': 1,\n",
       "             'stadium': 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_music_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_politic_word_count = find_number_words_in_category(test_statement,politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'world': 1,\n",
       "             'leaders': 1,\n",
       "             'agreed': 1,\n",
       "             'to': 0,\n",
       "             'fund': 0,\n",
       "             'the': 2,\n",
       "             'stadium': 0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_politic_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_likelihood(category_count,test_category_count,alpha):\n",
    "    # The numerator will be the product of all the counts \n",
    "    # with the smoothing factor (alpha) to make sure the probability is not zero'd out\n",
    "    num = np.product(np.array(list(test_category_count.values())) + alpha)\n",
    "    \n",
    "    # The denominator will be the same for each word (total category count + total vocab + alph)\n",
    "    # so we raise it to the power of the length of the test category\n",
    "    denom = (category_count + total_vocab_count*alpha)**(len(test_category_count))\n",
    "    \n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_m = find_likelihood(total_music_count,test_music_word_count,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_p = find_likelihood(total_politics_count,test_politic_word_count,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.107740405680756e-11\n",
      "1.748875897714495e-10\n"
     ]
    }
   ],
   "source": [
    "print(likelihood_m)\n",
    "print(likelihood_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $ P(politics | article) = P(politics) x \\prod_{i=1}^{d} P(word_{i} | politics) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deteriming the winner of our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/solvingforyhat.png\" width= \"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_politics = .5\n",
    "p_music = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(politics|article)  > p(music|article)\n",
    "likelihood_p * p_politics  > likelihood_m * p_music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times, the probabilities we end up are exceedingly small, so we can transform them using logs to save on computation speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large log(P(politics | article)) = log(P(politics)) + \\sum_{i=1}^{d}log( P(word_{i} | politics)) $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Resource: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.htmlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Satire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Noting that the resignation of James Mattis as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Desperate to unwind after months of nonstop wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Nearly halfway through his presidential term, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Attempting to make amends for gross abuses of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Decrying the Senate’s resolution blaming the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  target\n",
       "0  Noting that the resignation of James Mattis as...       1\n",
       "1  Desperate to unwind after months of nonstop wo...       1\n",
       "2  Nearly halfway through his presidential term, ...       1\n",
       "3  Attempting to make amends for gross abuses of ...       1\n",
       "4  Decrying the Senate’s resolution blaming the c...       1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "corpus = pd.read_csv('data/satire_nosatire.csv')\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like always, we will perform a train test split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=corpus.body\n",
    "y=corpus.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and preprocess the training set like we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our regex pattern that gets rid of numbers and punctuation\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "sw.extend(['would', 'one', 'say'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_preparer(doc, stop_words=sw):\n",
    "    '''\n",
    "    \n",
    "    :param doc: a document from the satire corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    \n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_docs = [doc_preparer(doc, sw) for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'perpetually offend social justice warrior keen activist socialist matter range lgbtqp affair feminism border soviet ideology marxism ideal communist state hugh mungus state perpetual offence wake morning first thing hear word good morning shout roommate form racist linguistic capitalist imperialist sexist white male create offensive greeting assumes good morning western capitalist bourgeois society people africa good morning shit morning live corrugate iron shack walk five hour fill bucket full dirty muddy water fuck drink mr mungus attend berkeley college california outrage deems society biology science offensive well concept offend offensive outrage offended offence offensive hateful perpetual state concept offend offend offended offensive stance state offence offend offend doubt mind creation capitalist racist sexist system imprisons people offend suspect create offence first place everything offensive perpetually state offence right offend concept offend offence offensive manner'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we will limit our count vectorizer to 5 words (the top 5 words by frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary train-test split to build our best model\n",
    "X_t, X_val, y_t, y_val = train_test_split(token_docs, y_train, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=5)\n",
    "\n",
    "# Just like with our scaler, we fit our Count Vectorizer on the training set\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people</th>\n",
       "      <th>say</th>\n",
       "      <th>state</th>\n",
       "      <th>trump</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>562 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     people  say  state  trump  year\n",
       "159       3    0      0      0     0\n",
       "246       1    0      0      7     1\n",
       "640       0    4      1      0     4\n",
       "809       2   10      2      0     7\n",
       "130       0    0      0      0     0\n",
       "..      ...  ...    ...    ...   ...\n",
       "148       1    0      0      0     1\n",
       "300       0    1      0      0     0\n",
       "356       1    3      0      0     0\n",
       "36        1    4      0      0     3\n",
       "895       1    7      0      0     6\n",
       "\n",
       "[562 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say comes up here because of the lemmatiser - the word 'said' would have come up and the lemmatiser turned it into 'say'.  So that's why it comes up even though 'say' is one of our stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Check\n",
    "\n",
    "The word say shows up in our count vectorizer, but it is excluded in the stopwords.  What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then transform the validation set.  Do not refit the vectorizer\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people</th>\n",
       "      <th>say</th>\n",
       "      <th>state</th>\n",
       "      <th>trump</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     people  say  state  trump  year\n",
       "267       0    2      0      0     0\n",
       "115       1    0      1      0     0\n",
       "234       0    0      0      0     0\n",
       "680       0    1      0      0     2\n",
       "126       0    0      0      0     1\n",
       "..      ...  ...    ...    ...   ...\n",
       "217       0    0      1      0     0\n",
       "190       2    0      3      0     0\n",
       "779       0    3      0      2     1\n",
       "971       1    6      0      0     1\n",
       "523       0   10      0      3     1\n",
       "\n",
       "[188 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the the Multinomial Naive Bayes Classifier on our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adam'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What should our priors for each class be?\n",
    "\n",
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142348754448398"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "prior_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48576512455516013"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_0 = y_t.value_counts()[0]/len(y_t)\n",
    "prior_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48576512455516013 0.5142348754448398\n",
      "-0.665075161781259\n"
     ]
    }
   ],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "prior_0 = y_t.value_counts()[0]/len(y_t)\n",
    "print(prior_0, prior_1)\n",
    "print(np.log(prior_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'fit_prior': True,\n",
       " 'class_prior': None,\n",
       " 'classes_': array([0, 1]),\n",
       " 'class_count_': array([273., 289.]),\n",
       " 'feature_count_': array([[ 211., 1419.,  371.,  283.,  348.],\n",
       "        [ 385.,  241.,  111.,  152.,  264.]]),\n",
       " 'feature_log_prob_': array([[-2.52081091, -0.61898504, -1.95850333, -2.22842295, -2.02232526],\n",
       "        [-1.09861229, -1.56551193, -2.33595079, -2.02401174, -1.47471983]]),\n",
       " 'class_log_prior_': array([-0.72203005, -0.66507516])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.72203005, -0.66507516])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8297872340425532"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the scenario that we would like to isolate satirical news on Facebook so we can flag it.  We do not want to flag real news by mistake. In other words, we want to minimize falls positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83, 16],\n",
       "       [16, 73]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8202247191011236"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good for a five word vocabulary.\n",
    "\n",
    "Let's see what happens when we increase don't restrict our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oday': 9217,\n",
       " 'practically': 10160,\n",
       " 'everyone': 4544,\n",
       " 'travelgoals': 13550,\n",
       " 'europe': 4514,\n",
       " 'first': 5003,\n",
       " 'travel': 13548,\n",
       " 'destination': 3519,\n",
       " 'goal': 5569,\n",
       " 'take': 13047,\n",
       " 'form': 5158,\n",
       " 'may': 8223,\n",
       " 'involve': 6906,\n",
       " 'many': 8101,\n",
       " 'possible': 10130,\n",
       " 'important': 6514,\n",
       " 'consideration': 2741,\n",
       " 'choice': 2238,\n",
       " 'budget': 1747,\n",
       " 'convenience': 2826,\n",
       " 'perhaps': 9787,\n",
       " 'proximity': 10435,\n",
       " 'look': 7851,\n",
       " 'place': 9963,\n",
       " 'stay': 12585,\n",
       " 'yes': 14737,\n",
       " 'traveler': 13549,\n",
       " 'consider': 2738,\n",
       " 'problem': 10316,\n",
       " 'course': 2941,\n",
       " 'pretty': 10258,\n",
       " 'fulfil': 5292,\n",
       " 'real': 10742,\n",
       " 'want': 14352,\n",
       " 'maybe': 8224,\n",
       " 'shouldn': 11974,\n",
       " 'limit': 7743,\n",
       " 'much': 8710,\n",
       " 'advance': 206,\n",
       " 'tip': 13368,\n",
       " 'though': 13298,\n",
       " 'you': 14758,\n",
       " 're': 10726,\n",
       " 'set': 11817,\n",
       " 'plan': 9971,\n",
       " 'get': 5485,\n",
       " 'expedia': 4623,\n",
       " 'deal': 3256,\n",
       " 'won': 14618,\n",
       " 'regret': 10883,\n",
       " 'fulfilling': 5294,\n",
       " 'people': 9770,\n",
       " 'tend': 13176,\n",
       " 'forget': 5153,\n",
       " 'need': 8898,\n",
       " 'dream': 3974,\n",
       " 'big': 1337,\n",
       " 'true': 13621,\n",
       " 'even': 4530,\n",
       " 'like': 7731,\n",
       " 'settle': 11821,\n",
       " 'le': 7566,\n",
       " 'content': 2795,\n",
       " 'trip': 13586,\n",
       " 'far': 4788,\n",
       " 'away': 963,\n",
       " 'give': 5523,\n",
       " 'experience': 4634,\n",
       " 'new': 8965,\n",
       " 'basically': 1143,\n",
       " 'rob': 11252,\n",
       " 'might': 8413,\n",
       " 'change': 2139,\n",
       " 'life': 7712,\n",
       " 'forever': 5150,\n",
       " 'sound': 12324,\n",
       " 'overly': 9487,\n",
       " 'dramatic': 3962,\n",
       " 'can': 1900,\n",
       " 'help': 6066,\n",
       " 'admit': 191,\n",
       " 'brave': 1623,\n",
       " 'bold': 1500,\n",
       " 'however': 6290,\n",
       " 'really': 10750,\n",
       " 'excellent': 4574,\n",
       " 'idea': 6396,\n",
       " 'try': 13630,\n",
       " 'visit': 14237,\n",
       " 'mean': 8250,\n",
       " 'spending': 12411,\n",
       " 'usually': 14040,\n",
       " 'spend': 12410,\n",
       " 'learn': 7584,\n",
       " 'cultural': 3103,\n",
       " 'information': 6686,\n",
       " 'usual': 14039,\n",
       " 'don': 3897,\n",
       " 'talk': 13058,\n",
       " 'mouth': 8680,\n",
       " 'full': 5295,\n",
       " 'sort': 12316,\n",
       " 'rule': 11366,\n",
       " 'definitely': 3344,\n",
       " 'ask': 782,\n",
       " 'leave': 7590,\n",
       " 'comfort': 2527,\n",
       " 'zone': 14812,\n",
       " 'delightful': 3384,\n",
       " 'way': 14399,\n",
       " 'make': 8028,\n",
       " 'worth': 14656,\n",
       " 'risk': 11232,\n",
       " 'reason': 10754,\n",
       " 'll': 7803,\n",
       " 'love': 7888,\n",
       " 'rich': 11186,\n",
       " 'historical': 6160,\n",
       " 'heritage': 6093,\n",
       " 'think': 13280,\n",
       " 'socrates': 12253,\n",
       " 'roman': 11286,\n",
       " 'coliseum': 2475,\n",
       " 'medieval': 8281,\n",
       " 'cathedral': 2039,\n",
       " 'common': 2556,\n",
       " 'probably': 10314,\n",
       " 'similarity': 12051,\n",
       " 'focus': 5095,\n",
       " 'they': 13271,\n",
       " 'see': 11712,\n",
       " 'somehow': 12289,\n",
       " 'screams': 11640,\n",
       " 'history': 6162,\n",
       " 'country': 2929,\n",
       " 'city': 2305,\n",
       " 'certainly': 2099,\n",
       " 'feel': 4883,\n",
       " 'breathe': 1649,\n",
       " 'ancient': 514,\n",
       " 'treat': 13559,\n",
       " 'aren': 698,\n",
       " 'likely': 7734,\n",
       " 'anywhere': 598,\n",
       " 'else': 4201,\n",
       " 'train': 13498,\n",
       " 'especially': 4457,\n",
       " 'lovable': 7887,\n",
       " 'happen': 5903,\n",
       " 'come': 2522,\n",
       " 'nearby': 8887,\n",
       " 'fly': 5091,\n",
       " 'joy': 7143,\n",
       " 'encounter': 4289,\n",
       " 'stark': 12556,\n",
       " 'difference': 3630,\n",
       " 'culture': 3105,\n",
       " 'language': 7498,\n",
       " 'simply': 12063,\n",
       " 'next': 8977,\n",
       " 'oh': 9249,\n",
       " 'view': 14180,\n",
       " 'spectacular': 12393,\n",
       " 'hour': 6278,\n",
       " 'ride': 11199,\n",
       " 'time': 13347,\n",
       " 'food': 5112,\n",
       " 'well': 14449,\n",
       " 'concerned': 2644,\n",
       " 'ever': 4537,\n",
       " 'compare': 2579,\n",
       " 'home': 6196,\n",
       " 'thing': 13278,\n",
       " 'nobody': 9041,\n",
       " 'necessarily': 8892,\n",
       " 'disagree': 3688,\n",
       " 'point': 10039,\n",
       " 'whatever': 14473,\n",
       " 'store': 12663,\n",
       " 'gastronomically': 5394,\n",
       " 'speak': 12375,\n",
       " 'it': 6973,\n",
       " 'small': 12182,\n",
       " 'offer': 9233,\n",
       " 'option': 9353,\n",
       " 'range': 10672,\n",
       " 'pizza': 9956,\n",
       " 'pasta': 9669,\n",
       " 'bread': 1635,\n",
       " 'seafood': 11668,\n",
       " 'warm': 14364,\n",
       " 'chocolate': 2237,\n",
       " 'eventually': 4536,\n",
       " 'realize': 10749,\n",
       " 'eat': 4083,\n",
       " 'chore': 2254,\n",
       " 'picturesque': 9904,\n",
       " 've': 14098,\n",
       " 'friend': 5250,\n",
       " 'colleague': 2483,\n",
       " 'opportunity': 9338,\n",
       " 'postcard': 10133,\n",
       " 'heard': 6009,\n",
       " 'story': 12667,\n",
       " 'witness': 14604,\n",
       " 'face': 4717,\n",
       " 'lit': 7778,\n",
       " 'remember': 10965,\n",
       " 'felt': 4892,\n",
       " 'european': 4515,\n",
       " 'bind': 1357,\n",
       " 'berlin': 1287,\n",
       " 'barcelona': 1104,\n",
       " 'florence': 5073,\n",
       " 'amsterdam': 491,\n",
       " 'name': 8821,\n",
       " 'truly': 13622,\n",
       " 'beyond': 1318,\n",
       " 'best': 1300,\n",
       " 'imaginings': 6458,\n",
       " 'art': 753,\n",
       " 'apart': 600,\n",
       " 'undeniably': 13778,\n",
       " 'also': 422,\n",
       " 'countless': 2928,\n",
       " 'museum': 8765,\n",
       " 'havens': 5972,\n",
       " 'preserve': 10239,\n",
       " 'nurture': 9142,\n",
       " 'creativity': 2991,\n",
       " 'kind': 7330,\n",
       " 'person': 9823,\n",
       " 'breathes': 1650,\n",
       " 'fresh': 5244,\n",
       " 'inspiration': 6754,\n",
       " 'without': 14602,\n",
       " 'doubt': 3930,\n",
       " 'music': 8767,\n",
       " 'hand': 5879,\n",
       " 'check': 2179,\n",
       " 'open': 9312,\n",
       " 'park': 9627,\n",
       " 'free': 5227,\n",
       " 'concert': 2645,\n",
       " 'hold': 6180,\n",
       " 'amazing': 447,\n",
       " 'performer': 9784,\n",
       " 'film': 4962,\n",
       " 'boast': 1479,\n",
       " 'feature': 4866,\n",
       " 'wide': 14522,\n",
       " 'space': 12350,\n",
       " 'watch': 14388,\n",
       " 'favorite': 4842,\n",
       " 'movie': 8684,\n",
       " 'impossible': 6522,\n",
       " 'artist': 761,\n",
       " 'surely': 12906,\n",
       " 'find': 4978,\n",
       " 'matter': 8204,\n",
       " 'temporary': 13171,\n",
       " 'conclusion': 2651,\n",
       " 'begin': 1221,\n",
       " 'note': 9085,\n",
       " 'go': 5568,\n",
       " 'vacation': 14060,\n",
       " 'boldly': 1501,\n",
       " 'choose': 2249,\n",
       " 'discussion': 3735,\n",
       " 'clearly': 2356,\n",
       " 'evolve': 4555,\n",
       " 'ultimately': 13732,\n",
       " 'present': 10234,\n",
       " 'enough': 4345,\n",
       " 'sooner': 12305,\n",
       " 'rather': 10705,\n",
       " 'later': 7523,\n",
       " 'end': 4296,\n",
       " 'final': 4969,\n",
       " 'decision': 3294,\n",
       " 'arm': 718,\n",
       " 'obviously': 9198,\n",
       " 'easy': 4082,\n",
       " 'across': 132,\n",
       " 'pond': 10079,\n",
       " 'somewhat': 12296,\n",
       " 'remove': 10979,\n",
       " 'hard': 5913,\n",
       " 'american': 463,\n",
       " 'socialist': 12243,\n",
       " 'fool': 5114,\n",
       " 'supposedly': 12895,\n",
       " 'inclusive': 6576,\n",
       " 'ideology': 6407,\n",
       " 'progressive': 10354,\n",
       " 'tolerance': 13401,\n",
       " 'yet': 14740,\n",
       " 'act': 133,\n",
       " 'fascist': 4814,\n",
       " 'non': 9051,\n",
       " 'intolerant': 6866,\n",
       " 'prone': 10376,\n",
       " 'censor': 2080,\n",
       " 'authoritarian': 915,\n",
       " 'totalitarian': 13447,\n",
       " 'increasingly': 6592,\n",
       " 'violent': 14213,\n",
       " 'hitler': 6166,\n",
       " 'proud': 10418,\n",
       " 'democrat': 3411,\n",
       " 'exhibit': 4602,\n",
       " 'exact': 4560,\n",
       " 'behaviour': 1229,\n",
       " 'nazi': 8877,\n",
       " 'party': 9652,\n",
       " 'leading': 7573,\n",
       " 'world': 14643,\n",
       " 'war': 14355,\n",
       " 'ii': 6431,\n",
       " 'national': 8854,\n",
       " 'socialism': 12242,\n",
       " 'wing': 14567,\n",
       " 'heavily': 6028,\n",
       " 'shut': 11993,\n",
       " 'speech': 12400,\n",
       " 'credo': 2999,\n",
       " 'hate': 5962,\n",
       " 'monger': 8591,\n",
       " 'mob': 8533,\n",
       " 'thug': 13321,\n",
       " 'cannot': 1922,\n",
       " 'discuss': 3732,\n",
       " 'anything': 595,\n",
       " 'civil': 2308,\n",
       " 'educate': 4126,\n",
       " 'manner': 8087,\n",
       " 'hatred': 5965,\n",
       " 'trump': 13623,\n",
       " 'creates': 2987,\n",
       " 'stuck': 12743,\n",
       " 'mindset': 8456,\n",
       " 'anger': 532,\n",
       " 'create': 2986,\n",
       " 'unwillingness': 13972,\n",
       " 'discus': 3731,\n",
       " 'acknowledge': 120,\n",
       " 'worldview': 14645,\n",
       " 'discord': 3719,\n",
       " 'alienation': 366,\n",
       " 'donald': 3898,\n",
       " 'backdrop': 1001,\n",
       " 'wail': 14322,\n",
       " 'whine': 14491,\n",
       " 'bully': 1764,\n",
       " 'civilised': 2313,\n",
       " 'baying': 1171,\n",
       " 'hound': 6277,\n",
       " 'answer': 565,\n",
       " 'race': 10608,\n",
       " 'bait': 1042,\n",
       " 'call': 1873,\n",
       " 'action': 135,\n",
       " 'maxine': 8222,\n",
       " 'water': 14391,\n",
       " 'civilise': 2312,\n",
       " 'peter': 9846,\n",
       " 'fonda': 5110,\n",
       " 'paedophile': 9543,\n",
       " 'rape': 10678,\n",
       " 'year': 14720,\n",
       " 'old': 9261,\n",
       " 'son': 12298,\n",
       " 'barron': 1129,\n",
       " 'administrative': 183,\n",
       " 'staff': 12520,\n",
       " 'restaurant': 11091,\n",
       " 'confront': 2686,\n",
       " 'outside': 9453,\n",
       " 'similar': 12050,\n",
       " 'jew': 7072,\n",
       " 'germany': 5475,\n",
       " 'turf': 13663,\n",
       " 'shop': 11958,\n",
       " 'public': 10452,\n",
       " 'positive': 10123,\n",
       " 'mature': 8210,\n",
       " 'exhibited': 4603,\n",
       " 'moment': 8574,\n",
       " 'frankly': 5218,\n",
       " 'bad': 1021,\n",
       " 'guy': 5809,\n",
       " 'everything': 4545,\n",
       " 'violence': 14212,\n",
       " 'crave': 2980,\n",
       " 'fare': 4794,\n",
       " 'fight': 4949,\n",
       " 'gun': 5792,\n",
       " 'know': 7379,\n",
       " 'shoot': 11953,\n",
       " 'straight': 12677,\n",
       " 'whereas': 14482,\n",
       " 'conservative': 2735,\n",
       " 'every': 4541,\n",
       " 'day': 3241,\n",
       " 'increase': 6590,\n",
       " 'strength': 12703,\n",
       " 'power': 10154,\n",
       " 'stature': 12580,\n",
       " 'whilst': 14488,\n",
       " 'dilapidate': 3646,\n",
       " 'silly': 12045,\n",
       " 'applies': 637,\n",
       " 'medium': 8284,\n",
       " 'company': 2577,\n",
       " 'hollywood': 6188,\n",
       " 'celebrity': 2070,\n",
       " 'pseudo': 10439,\n",
       " 'marxist': 8160,\n",
       " 'university': 13889,\n",
       " 'professor': 10339,\n",
       " 'indoctrinate': 6634,\n",
       " 'flock': 5067,\n",
       " 'bleed': 1424,\n",
       " 'heart': 6013,\n",
       " 'virtue': 14225,\n",
       " 'signal': 12026,\n",
       " 'miserable': 8486,\n",
       " 'insufferable': 6783,\n",
       " 'existence': 4608,\n",
       " 'sometimes': 12295,\n",
       " 'pay': 9708,\n",
       " 'step': 12608,\n",
       " 'back': 995,\n",
       " 'reality': 10748,\n",
       " 'manage': 8055,\n",
       " 'occur': 9208,\n",
       " 'nationally': 8860,\n",
       " 'globally': 5548,\n",
       " 'side': 12010,\n",
       " 'must': 8774,\n",
       " 'join': 7113,\n",
       " 'collectively': 2487,\n",
       " 'inclusively': 6577,\n",
       " 'second': 11689,\n",
       " 'thousand': 13302,\n",
       " 'march': 8111,\n",
       " 'budapest': 1744,\n",
       " 'centre': 2090,\n",
       " 'saturday': 11543,\n",
       " 'protest': 10413,\n",
       " 'law': 7544,\n",
       " 'allow': 398,\n",
       " 'employer': 4267,\n",
       " 'work': 14634,\n",
       " 'per': 9773,\n",
       " 'overtime': 9506,\n",
       " 'opposition': 9342,\n",
       " 'group': 5744,\n",
       " 'stag': 12524,\n",
       " 'several': 11830,\n",
       " 'rally': 10653,\n",
       " 'past': 9668,\n",
       " 'week': 14432,\n",
       " 'hungarian': 6330,\n",
       " 'capital': 1936,\n",
       " 'say': 11558,\n",
       " 'nationalist': 8857,\n",
       " 'viktor': 14194,\n",
       " 'orban': 9360,\n",
       " 'organise': 9377,\n",
       " 'trade': 13482,\n",
       " 'union': 13876,\n",
       " 'civic': 2307,\n",
       " 'mainly': 8011,\n",
       " 'target': 13085,\n",
       " 'labour': 7454,\n",
       " 'dub': 4012,\n",
       " 'critic': 3029,\n",
       " 'slave': 12140,\n",
       " 'protester': 10416,\n",
       " 'snowfall': 12227,\n",
       " 'historic': 6159,\n",
       " 'hero': 6096,\n",
       " 'square': 12485,\n",
       " 'parliament': 9630,\n",
       " 'build': 1754,\n",
       " 'bank': 1086,\n",
       " 'danube': 3206,\n",
       " 'river': 11238,\n",
       " 'carry': 1988,\n",
       " 'banner': 1092,\n",
       " 'sweep': 12966,\n",
       " 'regime': 10874,\n",
       " 'almost': 407,\n",
       " 'since': 12068,\n",
       " 'government': 5637,\n",
       " 'corruption': 2897,\n",
       " 'democracy': 3410,\n",
       " 'housewife': 6281,\n",
       " 'eva': 4520,\n",
       " 'demeter': 3406,\n",
       " 'pour': 10149,\n",
       " 'onto': 9300,\n",
       " 'street': 12701,\n",
       " 'affect': 236,\n",
       " 'crowd': 3055,\n",
       " 'post': 10132,\n",
       " 'social': 12240,\n",
       " 'strike': 12718,\n",
       " 'modification': 8555,\n",
       " 'code': 2451,\n",
       " 'pass': 9657,\n",
       " 'last': 7519,\n",
       " 'month': 8614,\n",
       " 'intense': 6811,\n",
       " 'criticism': 3034,\n",
       " 'spark': 12364,\n",
       " 'potentially': 10145,\n",
       " 'could': 2916,\n",
       " 'add': 156,\n",
       " 'two': 13703,\n",
       " 'extra': 4682,\n",
       " 'average': 944,\n",
       " 'equivalent': 4419,\n",
       " 'workday': 14635,\n",
       " 'zoltan': 14810,\n",
       " 'mucsi': 8711,\n",
       " 'head': 5991,\n",
       " 'steelmaker': 12597,\n",
       " 'dunaferr': 4034,\n",
       " 'vasas': 14092,\n",
       " 'undemocratic': 13777,\n",
       " 'main': 8009,\n",
       " 'resort': 11075,\n",
       " 'sit': 12091,\n",
       " 'negotiate': 8912,\n",
       " 'tell': 13162,\n",
       " 'reuters': 11139,\n",
       " 'membership': 8308,\n",
       " 'hungary': 6331,\n",
       " 'estimate': 4478,\n",
       " 'tenth': 13191,\n",
       " 'workforce': 14637,\n",
       " 'half': 5848,\n",
       " 'level': 7668,\n",
       " 'accord': 98,\n",
       " 'data': 3225,\n",
       " 'organisation': 9376,\n",
       " 'economic': 4105,\n",
       " 'co': 2424,\n",
       " 'operation': 9324,\n",
       " 'development': 3567,\n",
       " 'anti': 574,\n",
       " 'court': 2943,\n",
       " 'politically': 10062,\n",
       " 'manipulate': 8080,\n",
       " 'bias': 1327,\n",
       " 'state': 12569,\n",
       " 'control': 2817,\n",
       " 'earlier': 4063,\n",
       " 'mostly': 8652,\n",
       " 'peaceful': 9725,\n",
       " 'clash': 2331,\n",
       " 'police': 10053,\n",
       " 'use': 14028,\n",
       " 'tear': 13122,\n",
       " 'gas': 5390,\n",
       " 'ruling': 11369,\n",
       " 'fidesz': 4938,\n",
       " 'reiterate': 10915,\n",
       " 'statement': 12571,\n",
       " 'thursday': 13326,\n",
       " 'part': 9638,\n",
       " 'campaign': 1892,\n",
       " 'election': 4167,\n",
       " 'support': 12888,\n",
       " 'mass': 8168,\n",
       " 'migration': 8416,\n",
       " 'landslide': 7496,\n",
       " 'ticket': 13332,\n",
       " 'resist': 11068,\n",
       " 'immigration': 6475,\n",
       " 'eu': 4499,\n",
       " 'short': 11963,\n",
       " 'save': 11550,\n",
       " 'sink': 12080,\n",
       " 'economy': 4109,\n",
       " 'sudan': 12809,\n",
       " 'president': 10242,\n",
       " 'omar': 9281,\n",
       " 'al': 327,\n",
       " 'bashir': 1141,\n",
       " 'board': 1478,\n",
       " 'russian': 11395,\n",
       " 'jet': 7070,\n",
       " 'dec': 3274,\n",
       " 'become': 1201,\n",
       " 'arab': 667,\n",
       " 'leader': 7570,\n",
       " 'damascus': 3181,\n",
       " 'renew': 10989,\n",
       " 'push': 10519,\n",
       " 'financial': 4975,\n",
       " 'survive': 12927,\n",
       " 'event': 4533,\n",
       " 'soon': 12304,\n",
       " 'catch': 2031,\n",
       " 'demonstrator': 3426,\n",
       " 'price': 10274,\n",
       " 'rise': 11230,\n",
       " 'torch': 13431,\n",
       " 'headquarters': 5998,\n",
       " 'atbara': 844,\n",
       " 'quickly': 10583,\n",
       " 'spread': 12464,\n",
       " 'sustain': 12940,\n",
       " 'challenge': 2124,\n",
       " 'islamist': 6953,\n",
       " 'coup': 2932,\n",
       " 'nearly': 8889,\n",
       " 'ago': 278,\n",
       " 'widespread': 14526,\n",
       " 'longer': 7845,\n",
       " 'lasting': 7520,\n",
       " 'bout': 1587,\n",
       " 'unrest': 13934,\n",
       " 'september': 11787,\n",
       " 'january': 7031,\n",
       " 'propel': 10383,\n",
       " 'deep': 3317,\n",
       " 'crisis': 3024,\n",
       " 'upend': 13982,\n",
       " 'attempt': 867,\n",
       " 'stave': 12584,\n",
       " 'collapse': 2479,\n",
       " 'destabilise': 3516,\n",
       " 'beset': 1296,\n",
       " 'simmer': 12053,\n",
       " 'internal': 6843,\n",
       " 'conflict': 2683,\n",
       " 'straddle': 12675,\n",
       " 'africa': 253,\n",
       " 'middle': 8404,\n",
       " 'east': 4079,\n",
       " 'today': 13390,\n",
       " 'strong': 12726,\n",
       " 'wave': 14396,\n",
       " 'frustration': 5276,\n",
       " 'cost': 2904,\n",
       " 'living': 7800,\n",
       " 'boil': 1495,\n",
       " 'independent': 6611,\n",
       " 'analyst': 502,\n",
       " 'muhammad': 8720,\n",
       " 'osman': 9402,\n",
       " 'official': 9237,\n",
       " 'deaths': 3265,\n",
       " 'demonstration': 3425,\n",
       " 'amnesty': 481,\n",
       " 'international': 6845,\n",
       " 'credible': 2996,\n",
       " 'report': 11016,\n",
       " 'dead': 3249,\n",
       " 'inner': 6721,\n",
       " 'circle': 2289,\n",
       " 'region': 10878,\n",
       " 'long': 7843,\n",
       " 'serve': 11809,\n",
       " 'rid': 11195,\n",
       " 'sanction': 11495,\n",
       " 'shrug': 11990,\n",
       " 'indictment': 6621,\n",
       " 'criminal': 3017,\n",
       " 'charge': 2153,\n",
       " 'crimes': 3016,\n",
       " 'humanity': 6319,\n",
       " 'genocide': 5450,\n",
       " 'darfur': 3210,\n",
       " 'shore': 11962,\n",
       " 'position': 10122,\n",
       " 'economically': 4106,\n",
       " 'lobby': 7812,\n",
       " 'list': 7774,\n",
       " 'along': 412,\n",
       " 'syria': 13012,\n",
       " 'iran': 6923,\n",
       " 'north': 9069,\n",
       " 'korea': 7398,\n",
       " 'considers': 2742,\n",
       " 'sponsor': 12451,\n",
       " 'terrorism': 13212,\n",
       " 'prevent': 10261,\n",
       " 'influx': 6681,\n",
       " 'investment': 6896,\n",
       " 'aid': 299,\n",
       " 'hop': 6238,\n",
       " 'lifted': 7720,\n",
       " 'october': 9215,\n",
       " 'economist': 4108,\n",
       " 'instead': 6770,\n",
       " 'rapidly': 10684,\n",
       " 'expand': 4617,\n",
       " 'money': 8589,\n",
       " 'supply': 12887,\n",
       " 'finance': 4974,\n",
       " 'deficit': 3341,\n",
       " 'cause': 2047,\n",
       " 'spiral': 12432,\n",
       " 'inflation': 6675,\n",
       " 'officially': 9239,\n",
       " 'record': 10806,\n",
       " 'percent': 9775,\n",
       " 'steep': 12599,\n",
       " 'decline': 3300,\n",
       " 'value': 14074,\n",
       " 'currency': 3122,\n",
       " 'rehabilitate': 10892,\n",
       " 'ally': 404,\n",
       " 'bashar': 1140,\n",
       " 'assad': 793,\n",
       " 'order': 9366,\n",
       " 'win': 14557,\n",
       " 'play': 9988,\n",
       " 'seek': 11716,\n",
       " 'favour': 4843,\n",
       " 'gulf': 5791,\n",
       " 'include': 6573,\n",
       " 'saudi': 11544,\n",
       " 'arabia': 668,\n",
       " 'united': 13883,\n",
       " 'emirates': 4242,\n",
       " 'cut': 3140,\n",
       " 'tie': 13335,\n",
       " 'contribute': 2814,\n",
       " 'troop': 13599,\n",
       " 'lead': 7569,\n",
       " 'coalition': 2428,\n",
       " 'yemen': 14730,\n",
       " 'maintain': 8014,\n",
       " 'close': 2393,\n",
       " 'relation': 10923,\n",
       " 'regional': 10879,\n",
       " 'rival': 11236,\n",
       " 'qatar': 10542,\n",
       " 'turkey': 13666,\n",
       " 'confusion': 2691,\n",
       " 'sudanese': 12810,\n",
       " 'foreign': 5141,\n",
       " 'policy': 10055,\n",
       " 'shift': 11926,\n",
       " 'alliance': 393,\n",
       " 'another': 564,\n",
       " 'severe': 11831,\n",
       " 'obtain': 9195,\n",
       " 'quick': 10582,\n",
       " 'faisal': 4755,\n",
       " 'mohamed': 8564,\n",
       " 'saleh': 11461,\n",
       " 'journalist': 7135,\n",
       " 'political': 10061,\n",
       " 'khartoum': 7299,\n",
       " 'root': 11307,\n",
       " 'discontent': 3718,\n",
       " 'simmered': 12054,\n",
       " 'accelerate': 76,\n",
       " 'lose': 7869,\n",
       " 'three': 13305,\n",
       " 'quarter': 10564,\n",
       " 'oil': 9252,\n",
       " 'production': 10331,\n",
       " 'south': 12330,\n",
       " 'secede': 11686,\n",
       " 'eye': 4700,\n",
       " 'trouble': 13606,\n",
       " 'deeply': 3319,\n",
       " 'entwined': 4386,\n",
       " 'mismanagement': 8496,\n",
       " 'elite': 4191,\n",
       " 'congress': 2702,\n",
       " 'ncp': 8881,\n",
       " 'amjed': 477,\n",
       " 'farid': 4799,\n",
       " 'eltayeb': 4204,\n",
       " 'activist': 140,\n",
       " 'spokesman': 12448,\n",
       " 'movement': 8683,\n",
       " 'direct': 3676,\n",
       " 'security': 11707,\n",
       " 'apparatus': 617,\n",
       " 'militia': 8433,\n",
       " 'fund': 5304,\n",
       " 'service': 11812,\n",
       " 'education': 4127,\n",
       " 'health': 6004,\n",
       " 'protection': 10408,\n",
       " 'net': 8938,\n",
       " 'alone': 411,\n",
       " 'battle': 1165,\n",
       " 'fate': 4829,\n",
       " 'nothing': 9088,\n",
       " 'emasculate': 4215,\n",
       " 'leaderless': 7571,\n",
       " 'coalesce': 2427,\n",
       " 'individual': 6628,\n",
       " 'town': 13469,\n",
       " 'neighbourhood': 8921,\n",
       " 'circumvent': 2295,\n",
       " 'block': 1445,\n",
       " 'share': 11888,\n",
       " 'web': 14420,\n",
       " 'chant': 2142,\n",
       " 'line': 7755,\n",
       " 'famous': 4778,\n",
       " 'spring': 12468,\n",
       " 'fall': 4764,\n",
       " 'continue': 2806,\n",
       " 'despite': 3511,\n",
       " 'force': 5130,\n",
       " 'live': 7792,\n",
       " 'ammunition': 480,\n",
       " 'stun': 12751,\n",
       " 'grenade': 5713,\n",
       " 'disperse': 3787,\n",
       " 'independence': 6610,\n",
       " 'ruler': 11368,\n",
       " 'topple': 13430,\n",
       " 'eight': 4146,\n",
       " 'unless': 13895,\n",
       " 'switch': 12984,\n",
       " 'candidate': 1912,\n",
       " 'ahead': 292,\n",
       " 'due': 4019,\n",
       " 'zine': 14804,\n",
       " 'abedine': 27,\n",
       " 'science': 11615,\n",
       " 'lessen': 7660,\n",
       " 'chance': 2136,\n",
       " 'run': 11374,\n",
       " 'show': 11977,\n",
       " 'sign': 12025,\n",
       " 'aside': 781,\n",
       " 'member': 8307,\n",
       " 'dominate': 3893,\n",
       " 'propose': 10393,\n",
       " 'constitutional': 2766,\n",
       " 'amendment': 460,\n",
       " 'extend': 4669,\n",
       " 'term': 13199,\n",
       " 'require': 11041,\n",
       " 'appear': 622,\n",
       " 'largely': 7510,\n",
       " 'unperturbed': 13915,\n",
       " 'temper': 13165,\n",
       " 'conspiracy': 2753,\n",
       " 'pledge': 10002,\n",
       " 'reform': 10855,\n",
       " 'restraint': 11097,\n",
       " 'announcement': 553,\n",
       " 'fact': 4725,\n",
       " 'committee': 2554,\n",
       " 'dismiss': 3772,\n",
       " 'symbolic': 13000,\n",
       " 'commit': 2550,\n",
       " 'fair': 4748,\n",
       " 'prepare': 10223,\n",
       " 'participate': 9642,\n",
       " 'monday': 8585,\n",
       " 'write': 14677,\n",
       " 'aidan': 300,\n",
       " 'lewis': 7673,\n",
       " 'additional': 163,\n",
       " 'reporting': 11020,\n",
       " 'rinat': 11222,\n",
       " 'sagdiev': 11440,\n",
       " 'moscow': 8646,\n",
       " 'edit': 4120,\n",
       " 'giles': 5511,\n",
       " 'elgood': 4184,\n",
       " 'quote': 10598,\n",
       " 'delay': 3364,\n",
       " 'minimum': 8464,\n",
       " 'minute': 8474,\n",
       " 'complete': 2605,\n",
       " 'exchange': 4579,\n",
       " 'right': 11211,\n",
       " 'reserve': 11054,\n",
       " 'ots': 9410,\n",
       " 'user': 14032,\n",
       " 'error': 4445,\n",
       " 'application': 636,\n",
       " 'fail': 4743,\n",
       " 'essential': 4467,\n",
       " 'component': 2617,\n",
       " 'receive': 10775,\n",
       " 'message': 8350,\n",
       " 'search': 11677,\n",
       " 'effective': 4131,\n",
       " 'fix': 5018,\n",
       " 'api': 606,\n",
       " 'crt': 3059,\n",
       " 'runtime': 11380,\n",
       " 'dll': 3855,\n",
       " 'miss': 8500,\n",
       " 'responsible': 11086,\n",
       " 'moodiness': 8619,\n",
       " 'critical': 3030,\n",
       " 'method': 8368,\n",
       " 'solve': 12284,\n",
       " 'hope': 6239,\n",
       " 'useful': 14030,\n",
       " 'dynamic': 4055,\n",
       " 'link': 7762,\n",
       " 'library': 7699,\n",
       " 'dlls': 3856,\n",
       " 'external': 4676,\n",
       " 'window': 14561,\n",
       " 'storage': 12661,\n",
       " 'piece': 9906,\n",
       " 'program': 10347,\n",
       " 'computer': 2630,\n",
       " 'memory': 8315,\n",
       " 'certain': 2098,\n",
       " 'consequently': 2732,\n",
       " 'corrupt': 2896,\n",
       " 'completely': 2606,\n",
       " 'do': 3860,\n",
       " 'address': 166,\n",
       " 'immediate': 6470,\n",
       " 'reaction': 10729,\n",
       " 'system': 13015,\n",
       " 'urge': 14009,\n",
       " 'download': 3942,\n",
       " 'deem': 3314,\n",
       " 'internet': 6848,\n",
       " 'recommend': 10798,\n",
       " 'restart': 11090,\n",
       " 'often': 9247,\n",
       " 'pc': 9719,\n",
       " 'react': 10728,\n",
       " 'adequately': 170,\n",
       " 'command': 2533,\n",
       " 'case': 2003,\n",
       " 'reboot': 10766,\n",
       " 'solution': 12283,\n",
       " 'update': 13979,\n",
       " 'os': 9398,\n",
       " 'late': 7521,\n",
       " 'version': 14138,\n",
       " 'majority': 8025,\n",
       " 'situation': 12098,\n",
       " 'outdated': 9428,\n",
       " 'unable': 13742,\n",
       " 'recently': 10781,\n",
       " 'windows': 14562,\n",
       " 'compatible': 2583,\n",
       " 'therefore': 13267,\n",
       " 'updated': 13980,\n",
       " 'substantially': 12787,\n",
       " 'reduce': 10833,\n",
       " 'different': 3632,\n",
       " 'upgrade': 13984,\n",
       " 'sure': 12905,\n",
       " 'follow': 5106,\n",
       " 'start': 12559,\n",
       " 'menu': 8328,\n",
       " 'panel': 9577,\n",
       " 'select': 11730,\n",
       " 'available': 937,\n",
       " 'install': 6762,\n",
       " 'confirm': 2678,\n",
       " 'click': 2366,\n",
       " 'ok': 9255,\n",
       " 'wait': 14325,\n",
       " 'comp': 2574,\n",
       " 'still': 12625,\n",
       " 'persist': 9819,\n",
       " 'file': 4957,\n",
       " 'necessary': 8893,\n",
       " 'document': 3867,\n",
       " 'recycle': 10824,\n",
       " 'bin': 1355,\n",
       " 'delete': 3369,\n",
       " 'inadvertently': 6551,\n",
       " 'bring': 1683,\n",
       " 'lots': 7876,\n",
       " 'fortunately': 5177,\n",
       " 'always': 439,\n",
       " 'recover': 10813,\n",
       " 'highlight': 6125,\n",
       " 'restore': 11096,\n",
       " 'care': 1958,\n",
       " 'discover': 3724,\n",
       " 'special': 12381,\n",
       " 'software': 12260,\n",
       " 'lot': 7875,\n",
       " 'online': 9296,\n",
       " 'source': 12328,\n",
       " 'avail': 936,\n",
       " 'nonetheless': 9053,\n",
       " 'safe': 11433,\n",
       " ...}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[96,  3],\n",
       "       [ 4, 85]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "confusion_matrix(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Look how well that performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9659090909090909"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14819"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether or not we can maintain that level of accuracy with less words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9431818181818182"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=.05, max_df=.95)\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "precision_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see what happens with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "precision_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF does not necessarily perform better than CV.  It is just a tool in our toolbelt which we can try out and compare the performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14819"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9651162790697675"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=.05, max_df=.95)\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "precision_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare MNB to one of our classifiers that has a track record of high performance, Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946236559139785"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_features=5, max_depth=5)\n",
    "rf.fit(X_t_vec, y_t)\n",
    "y_hat = rf.predict(X_val_vec)\n",
    "precision_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both random forest and mnb perform comparably, however, mnb is lightweight as far as computational power and speed.  For real time predictions, we may choose MNB over random forest because the classifications can be performed quickly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
